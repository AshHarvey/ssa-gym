import gym
import ray.utils
from ray.rllib.models.preprocessors import get_preprocessor
from ray.rllib.evaluation.sample_batch_builder import SampleBatchBuilder
from ray.rllib.offline.json_writer import JsonWriter
import numpy as np
from tqdm import tqdm
from agents import agent_visible_greedy, agent_visible_greedy_spoiled, agent_naive_random
from envs import env_config

"""
The purpose of this script is to create a batch of offline trajectories with a set of agents and save those experiences
to disk in the RLLib json format. These trajectories are later used to train the imitation learning algorithm.
"""

ray.init()
#!------------ Configure the experiences to write to disk
agents = [agent_visible_greedy, agent_visible_greedy_spoiled, agent_naive_random]
rso_count = [10, 20, 40]
env_config['obs_returned'] = 'flatten'
env_config['reward_type'] = 'jones'
episodes = 10000

#!------------ Save experiences generated by the agent
for agent in agents:
    for m in rso_count:
        env_config['rso_count']  = m
        batch_builder = SampleBatchBuilder()  # or MultiAgentSampleBatchBuilder
        writer = JsonWriter('/home/ash/ray_results/ssa_experiences/' + agent.__name__ + '/' + str(env_config['rso_count'])
                            + 'RSOs_' + env_config['reward_type'] + '_' + env_config['obs_returned'] + '_' + str(episodes)
                            + 'episodes')

        # You normally wouldn't want to manually create sample batches if a
        # simulator is available, but let's do it anyways for example purposes:
        env = gym.make('ssa_tasker_simple-v2', **{'config': env_config})

        # RLlib uses preprocessors to implement transforms such as one-hot encoding
        # and flattening of tuple and dict observations. For CartPole a no-op
        # preprocessor is used, but this may be relevant for more complex envs.
        prep = get_preprocessor(env.observation_space)(env.observation_space)
        print("The preprocessor is", prep)
        for eps_id in tqdm(range(episodes)):
            obs = env.reset()
            prev_action = np.zeros_like(env.action_space.sample())
            prev_reward = 0
            done = False
            t = 0
            while not done:
                action = agent(obs, env)
                new_obs, rew, done, info = env.step(action)
                batch_builder.add_values(
                    t=t,
                    eps_id=eps_id,
                    agent_index=0,
                    obs=prep.transform(obs),
                    actions=action,
                    action_prob=1.0,  # put the true action probability here
                    rewards=rew,
                    prev_actions=prev_action,
                    prev_rewards=prev_reward,
                    dones=done,
                    infos=info,
                    new_obs=prep.transform(new_obs))
                obs = new_obs
                prev_action = action
                prev_reward = rew
                t += 1
            writer.write(batch_builder.build_and_reset())

# '/home/ash/ray_results/ssa_experiences/agent_visible_greedy/10RSOs_jones_flatten_10000episodes/'
# '/home/ash/ray_results/ssa_experiences/agent_visible_greedy/20RSOs_jones_flatten_10000episodes/'
# '/home/ash/ray_results/ssa_experiences/agent_visible_greedy/40RSOs_jones_flatten_10000episodes/'

# '/home/ash/ray_results/ssa_experiences/agent_visible_greedy_spoiled/10RSOs_jones_flatten_10000episodes/'
# '/home/ash/ray_results/ssa_experiences/agent_visible_greedy_spoiled/20RSOs_jones_flatten_10000episodes/'
# '/home/ash/ray_results/ssa_experiences/agent_visible_greedy_spoiled/40RSOs_jones_flatten_10000episodes/'

# '/home/ash/ray_results/ssa_experiences/agent_naive_random/10RSOs_jones_flatten_10000episodes/'
# '/home/ash/ray_results/ssa_experiences/agent_naive_random/20RSOs_jones_flatten_10000episodes/'
# '/home/ash/ray_results/ssa_experiences/agent_naive_random/40RSOs_jones_flatten_10000episodes/'
